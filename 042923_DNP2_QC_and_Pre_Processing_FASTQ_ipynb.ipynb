{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdgdYH9s/D8x2FcfOPasrB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DPariser/DataScience/blob/main/042923_DNP2_QC_and_Pre_Processing_FASTQ_ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plese DO NOT RUN THIS\n",
        "## Re-running this will override the files existing in the google drive. Please use as a VIEW ONLY."
      ],
      "metadata": {
        "id": "SysE8Aj-h45r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Environment\n",
        "\n",
        "This Notebook is created fresh with nothing else installed explicitly besides what is shown. So we assume that if you follow the instruction exactly, it should run out of the box."
      ],
      "metadata": {
        "id": "qxENFfu4h65m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1TQrzCYBh3Nr"
      },
      "outputs": [],
      "source": [
        "# This is  used to time the running of the notebook\n",
        "import time\n",
        "start_time = time.time()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# These packages are pre-installed on Google Colab, but are included here to simplify running this notebook locally\n",
        "%%capture\n",
        "!pip install matplotlib\n",
        "!pip install scikit-learn\n",
        "!pip install numpy\n",
        "!pip install scipy\n",
        "!pip install scanpy\n",
        "!pip install anndata\n",
        "!pip3 install leidenalg"
      ],
      "metadata": {
        "id": "KugM8-Bnh86w"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install packages for analysis and plotting\n",
        "from scipy.io import mmread\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import os\n",
        "import scanpy as sc\n",
        "import anndata\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "from scipy.sparse import csr_matrix\n",
        "matplotlib.rcParams.update({'font.size': 22})\n",
        "%config InlineBackend.figure_format = 'retina'"
      ],
      "metadata": {
        "id": "6ETpYIzGh-v6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "%%capture\n",
        "# `kb` is a wrapper for the kallisto and bustools program, and the kb-python package contains the kallisto and bustools executables.\n",
        "!pip install kb-python==0.24.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVdwkRctiCCB",
        "outputId": "68994c35-f6cc-44de-cbd2-7a59e5cf0fb9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 151 ms, sys: 18.6 ms, total: 170 ms\n",
            "Wall time: 16.8 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚ùó**Connect to the Data**\n",
        "\n",
        "The data is stored on a shared location in Google Drive. Since many of the files are very large and thus it is not feasable to download them to a location and use them. One good way of dealing with this situation is to create a shortcut to your own Google Drive and point to the shortcut and use them just like they are your own files on Google Drive. Here is the instruction how to set this up.\n",
        "\n",
        "* Click on the link to the share location of the data.\n",
        "* Nevigate to the \"Data files\" folder.\n",
        "* Click on the \"Dropdown\" arrow right next to the breaksrumb on the top right.\n",
        "* Choose \"Add shortcut to Drive\".\n",
        "\n",
        "Now it should appear in your Google Drive as the \"Data files\" folder.\n",
        "You can now connect to your Google Drive and access the file.\n",
        "From this point on, we assume that you have the Google Drive setup this way.\n",
        "\n",
        "Let's mount the Google Drive:"
      ],
      "metadata": {
        "id": "-m_vM032iGpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTaapee8iIpJ",
        "outputId": "db4a87a6-0d92-4d09-ef8c-19daa9198b1b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Google drive root\n",
        "gd_root = \"/content/drive/MyDrive/Pate_Lab/DNP/Bioinformatics\"\n",
        "\n",
        "# Data roots\n",
        "patient_root = f\"{gd_root}/H17_LungMk/Data_files/HRA001149/HRR339729\"\n",
        "lungmk_root = f\"{gd_root}/H17_LungMk/LungMk\"\n",
        "\n",
        "# Working directories\n",
        "patient_dir = f\"{patient_root}\"\n",
        "lungmk_dir = f\"{lungmk_root}\"\n",
        "\n",
        "# Create the directories if they don't exist\n",
        "!mkdir -p \"{patient_dir}\"\n",
        "!mkdir -p \"{lungmk_dir}\"\n",
        "\n",
        "# List the contents of the directories\n",
        "print(\"Contents of patient directory:\")\n",
        "!ls \"{patient_dir}\"\n",
        "print(\"\\nContents of LungMk directory:\")\n",
        "!ls \"{lungmk_dir}\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JM656jm3iZ4f",
        "outputId": "6fd13b8d-dae1-4c58-932e-64e91e88a421"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of patient directory:\n",
            "10xv2_whitelist.txt\t\t HRR339729_r2.fastq.gz\toutput.bus\n",
            "counts_unfiltered\t\t HRR339729_sta.xml\toutput.unfiltered.bus\n",
            "filtered_normalized_counts.h5ad  inspect.json\t\trun_info.json\n",
            "HRR339729_f1.fastq.gz\t\t matrix.ec\t\ttranscripts.txt\n",
            "\n",
            "Contents of LungMk directory:\n",
            "10xv2_whitelist.txt  inspect.json\t    run_info.json\n",
            "counts_unfiltered    matrix.ec\t\t    t2g.txt\n",
            "GRCh38genome.idx     output.bus\t\t    transcripts.txt\n",
            "index.idx\t     output.unfiltered.bus  v1nm7lpnqz5syh8dyzdk2zs8bglncfib.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if the directories exist\n",
        "if os.path.exists(lungmk_dir):\n",
        "    print(f\"The directory {lungmk_dir} exists.\")\n",
        "else:\n",
        "    print(f\"The directory {lungmk_dir} does not exist.\")\n",
        "\n",
        "if os.path.exists(patient_dir):\n",
        "    print(f\"The directory {patient_dir} exists.\")\n",
        "else:\n",
        "    print(f\"The directory {patient_dir} does not exist.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMLC7iikicEQ",
        "outputId": "f754ec34-ca88-48cf-97da-ad6393b9fba9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The directory /content/drive/MyDrive/Pate_Lab/DNP/Bioinformatics/H17_LungMk/LungMk exists.\n",
            "The directory /content/drive/MyDrive/Pate_Lab/DNP/Bioinformatics/H17_LungMk/Data_files/HRA001149/HRR339729 exists.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quality Control\n",
        "\n",
        "## need to edit this for looping\n",
        "\n",
        "* https://colab.research.google.com/github/pachterlab/kallistobustools/blob/master/docs/tutorials/kb_getting_started/python/kb_intro_2_python.ipynb\n",
        "\n",
        "## Filtering cells based on count\n",
        "Preliminary counts were then used for downstream analysis. Quality control was applied to cells based on three metrics step by step: the total UMI counts, number of detected genes and proportion of mitochondrial gene counts per cell. Specifically, cells with less than 1500 UMI counts and 500 detected genes were filtered, as well as cells with more than 10% mitochondrial gene counts. \n",
        "\n",
        "## Remove potential doublets (double balloon effect)\n",
        "\n",
        "This is what the investigators did in the original paper:\n",
        "\n",
        "\n",
        "*   To remove potential doublets, for PBMC samples, cells with UMI counts above 25,000 and detected genes above 5,000 are filtered out. For other tissues, cells with UMI counts above 70,000 and detected genes above 7,500 are filtered out. Additionally, we applied Scrublet (Wolock et al., 2019 link text) to identify potential doublets. The doublet score for each single cell and the threshold based on the bimodal distribution was calculated using default parameters. The expected doublet rate was set to be 0.08, and cells predicted to be doublets or with doubletScore larger than 0.25 were filtered. After quality control, a total of 1,598,708 cells were remained.\n",
        "*   for now we will not be using onliy the PBMC filter methods applied to all tissues\n",
        "*  *We may revisit this later*\n",
        "\n",
        "## Normalize\n",
        "For normalization of UMI counts, the Scanpy package provides several normalization methods, including the Total Count Normalization (TCN) and Normalization by Logarithm (LogNormalize) methods, which are commonly used in single-cell RNA-seq analysis. Here, we first load the count matrix using Scanpy's read_text function. We then normalize the data using the normalize_total function, which scales the counts for each cell so that they have the same total count (in this case, 10,000). We then scale the data by cell-specific size factors using the scale function, and logarithmically transform the data using the log1p function."
      ],
      "metadata": {
        "id": "psqPuQKaif3I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DO NOT RUN THIS UNLESS NEEDED!\n",
        "This code is to delete the post-processed .ha5d file if a new one needs to be created for any reason"
      ],
      "metadata": {
        "id": "SWZi6HsSiiZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Google drive root\n",
        "gd_root = \"/content/drive/MyDrive/Pate_Lab/DNP/Bioinformatics\"\n",
        "\n",
        "# Data roots\n",
        "data_root = f\"{gd_root}/H17_LungMk/Data_files/HRA001149\"\n",
        "lungmk_root = f\"{gd_root}/H17_LungMk/LungMk\"\n",
        "\n",
        "# Working directories\n",
        "data_dir = f\"{data_root}\"\n",
        "lungmk_dir = f\"{lungmk_root}\"\n",
        "\n",
        "# Create the directories if they don't exist\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "os.makedirs(lungmk_dir, exist_ok=True)\n",
        "\n",
        "# List the contents of the directories\n",
        "print(\"Contents of data directory:\")\n",
        "!ls \"{data_dir}\"\n",
        "print(\"\\nContents of LungMk directory:\")\n",
        "!ls \"{lungmk_dir}\"\n",
        "\n",
        "# Loop through the patients\n",
        "patient_ids = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
        "\n",
        "for patient_id in patient_ids:\n",
        "    # Patient directories\n",
        "    patient_root = f\"{data_dir}/{patient_id}\"\n",
        "    patient_dir = f\"{patient_root}\"\n",
        "    \n",
        "    # Check if the file exists and delete it\n",
        "    filtered_normalized_counts_file = f\"{patient_dir}/filtered_normalized_counts.h5ad\"\n",
        "    if os.path.exists(filtered_normalized_counts_file):\n",
        "        print(f\"Deleting {filtered_normalized_counts_file}\")\n",
        "        os.remove(filtered_normalized_counts_file)\n",
        "        print(\"File deleted.\")\n",
        "    else:\n",
        "        print(f\"File {filtered_normalized_counts_file} does not exist.\")\n"
      ],
      "metadata": {
        "id": "fTZG5od9ikHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DO NOT RUN THIS UNLESS NEEDED!\n",
        "This code is to delete the post-processed .ha5d file if a new one needs to be created for any reason"
      ],
      "metadata": {
        "id": "uMxDpCx3ik5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Identify all the patients so we can loop through them in the cell below\n",
        "folder_path = \"/content/drive/MyDrive/Pate_Lab/DNP/Bioinformatics/H17_LungMk/Data_files/HRA001149\"\n",
        "\n",
        "# List the directories in the folder\n",
        "directories = [d for d in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, d))]\n",
        "\n",
        "# Print the directories\n",
        "print(directories)\n",
        "\n",
        "# Use the directories as patient_ids\n",
        "patient_ids = directories\n",
        "\n",
        "for patient_id in patient_ids:\n",
        "    patient_root = f\"{folder_path}/{patient_id}\"\n",
        "    patient_dir = f\"{patient_root}\"\n",
        "    \n",
        "    # Create the directory if it doesn't exist\n",
        "    !mkdir -p \"{patient_dir}\"\n",
        "    \n",
        "    # Check if the directory exists\n",
        "    if os.path.exists(patient_dir):\n",
        "        print(f\"The directory {patient_dir} exists.\")\n",
        "    else:\n",
        "        print(f\"The directory {patient_dir} does not exist.\")\n",
        "        \n",
        "# Function to filter and normalize the data based on the given criteria\n",
        "def filter_and_normalize_data(patient_dir):\n",
        "    try:\n",
        "        adata = sc.read_mtx(f\"{patient_dir}/counts_unfiltered/cells_x_genes.mtx\")\n",
        "        adata.var_names = pd.read_csv(f\"{patient_dir}/counts_unfiltered/cells_x_genes.genes.txt\", header=None, sep='\\t')[0]\n",
        "        adata.obs_names = pd.read_csv(f\"{patient_dir}/counts_unfiltered/cells_x_genes.barcodes.txt\", header=None)[0]\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found error encountered for patient data in {patient_dir}. Skipping.\")\n",
        "        return\n",
        "    \n",
        "    print(f\"Initial number of cells: {adata.shape[0]}, Initial number of genes: {adata.shape[1]}\")\n",
        "    \n",
        "    # Filter cells based on total UMI counts, number of detected genes, and proportion of mitochondrial gene counts per cell\n",
        "    sc.pp.filter_cells(adata, min_counts=1500)\n",
        "    sc.pp.filter_cells(adata, min_genes=500)\n",
        "    mito_genes = adata.var_names.str.startswith('MT-')\n",
        "    adata.obs['percent_mito'] = np.sum(adata[:, mito_genes].X, axis=1).A1 / np.sum(adata.X, axis=1).A1\n",
        "    adata = adata[adata.obs['percent_mito'] < 0.1, :]\n",
        "    \n",
        "    print(f\"Number of cells after filtering by UMI counts, detected genes, and mitochondrial gene counts: {adata.shape[0]}\")\n",
        "    \n",
        "    # Filter cells based on potential doublets\n",
        "    sc.pp.filter_cells(adata, max_counts=25000)\n",
        "    sc.pp.filter_cells(adata, max_genes=5000)\n",
        "    \n",
        "    print(f\"Number of cells after filtering by potential doublets: {adata.shape[0]}\")\n",
        "    \n",
        "    if adata.shape[0] == 0:\n",
        "        print(f\"No cells remaining after filtering for patient data in {patient_dir}. Skipping.\")\n",
        "        return\n",
        "    \n",
        "    try:\n",
        "        # Normalize the data using Total Count Normalization (TCN) method\n",
        "        sc.pp.normalize_total(adata, target_sum=1e4)\n",
        "\n",
        "        # Logarithmically transform the data\n",
        "        sc.pp.log1p(adata)\n",
        "\n",
        "        # Scale the data by cell-specific size factors\n",
        "        sc.pp.scale(adata)\n",
        "\n",
        "    except ZeroDivisionError:\n",
        "        print(f\"ZeroDivisionError encountered for patient data in {patient_dir}. Skipping.\")\n",
        "        return\n",
        "    \n",
        "    # Calculate the remaining number of cells after filtering\n",
        "    num_cells = adata.shape[0]\n",
        "    print(f\"Number of cells remaining after filtering: {num_cells}\")\n",
        "\n",
        "    # Save the filtered and normalized data in the patient's original folder\n",
        "    adata.write(f\"{patient_dir}/filtered_normalized_counts.h5ad\")\n",
        "\n",
        "# Loop through each patient and filter and normalize the data\n",
        "for patient_id in patient_ids:\n",
        "    patient_root = f\"{folder_path}/{patient_id}\"\n",
        "    print(f\"Processing patient {patient_id}\")\n",
        "\n",
        "    # Check if the filtered and normalized counts file already exists\n",
        "    filtered_normalized_counts_file = f\"{patient_root}/filtered_normalized_counts.h5ad\"\n",
        "    if os.path.exists(filtered_normalized_counts_file):\n",
        "        print(f\"Filtered and normalized counts file for patient {patient_id} already exists. Skipping.\")\n",
        "        continue\n",
        "\n",
        "    # If the filtered and normalized counts file doesn't exist, process, filter and normalize the data\n",
        "    filter_and_normalize_data(patient_root)"
      ],
      "metadata": {
        "id": "Ax6As8dginRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Combine all patient files into one combine data file (.ha5d)"
      ],
      "metadata": {
        "id": "yq5_1JeJio7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import scanpy as sc\n",
        "import anndata\n",
        "\n",
        "# Identify all the patients so we can loop through them\n",
        "folder_path = \"/content/drive/MyDrive/Pate_Lab/DNP/Bioinformatics/H17_LungMk/Data_files/HRA001149\"\n",
        "\n",
        "# List the directories in the folder\n",
        "directories = [d for d in os.listdir(folder_path) if os.path.isdir(os.path.join(folder_path, d))]\n",
        "\n",
        "# Use the directories as patient_ids\n",
        "patient_ids = directories\n",
        "\n",
        "chunk_size = 5  # Define the size of the chunks\n",
        "chunks = [patient_ids[i:i + chunk_size] for i in range(0, len(patient_ids), chunk_size)]\n",
        "\n",
        "combined_data = None\n",
        "\n",
        "for i, chunk in enumerate(chunks):\n",
        "    adatas = []\n",
        "    for patient_id in chunk:\n",
        "        patient_root = f\"{folder_path}/{patient_id}\"\n",
        "        patient_dir = f\"{patient_root}\"\n",
        "\n",
        "        # Load the filtered and normalized data for each patient\n",
        "        filtered_normalized_file = f\"{patient_dir}/filtered_normalized_counts.h5ad\"\n",
        "        try:\n",
        "            adata = sc.read(filtered_normalized_file)  # Load data into memory\n",
        "        except FileNotFoundError:\n",
        "            print(f\"File not found error for patient {patient_id}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Append the adata to the list\n",
        "        adatas.append(adata)\n",
        "\n",
        "    # Concatenate the Anndata objects\n",
        "    chunk_data = anndata.concat(adatas, join='outer')\n",
        "\n",
        "    # If this is the first chunk, assign it to combined_data\n",
        "    if combined_data is None:\n",
        "        combined_data = chunk_data\n",
        "    else:\n",
        "        # If this is not the first chunk, concatenate it with the existing combined_data\n",
        "        combined_data = anndata.concat([combined_data, chunk_data], join='outer')\n",
        "\n",
        "    # Delete the adatas and chunk_data variables to free up memory\n",
        "    del adatas\n",
        "    del chunk_data\n",
        "\n",
        "# Save the combined data to a file\n",
        "combined_data_file = \"/content/drive/MyDrive/Pate_Lab/DNP/Bioinformatics/H17_LungMk/Data_files/combined_data.h5ad\"\n",
        "combined_data.write(combined_data_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzgNE_SLirJd",
        "outputId": "06d9585f-c75e-4e02-9ef0-33a5a51a09c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/anndata/_core/anndata.py:1830: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
            "  utils.warn_names_duplicates(\"obs\")\n",
            "/usr/local/lib/python3.10/dist-packages/anndata/_core/anndata.py:1830: UserWarning: Observation names are not unique. To make them unique, call `.obs_names_make_unique`.\n",
            "  utils.warn_names_duplicates(\"obs\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perform a QC check of the counts post-filtering"
      ],
      "metadata": {
        "id": "HiwhibL1iyZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate violin plots for combined data\n",
        "sc.pl.violin(combined_data, ['n_genes', 'n_counts', 'percent_mito'], jitter=0.4, multi_panel=True)"
      ],
      "metadata": {
        "id": "b9yvGwidizGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatter plot: n_counts vs. percent_mito\n",
        "sc.pl.scatter(combined_data, x='n_counts', y='percent_mito')\n",
        "\n",
        "# Scatter plot: n_counts vs. n_genes\n",
        "sc.pl.scatter(combined_data, x='n_counts', y='n_genes')"
      ],
      "metadata": {
        "id": "CVl8ycUyi1U4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}